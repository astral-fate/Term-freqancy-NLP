{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Natural Language Processing (NLP) is a sub-field of artificial intelligence that deals understanding and processing human language. In light of new advancements in machine learning, many organizations have begun applying natural language processing for translation, chatbots and candidate filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 =  \"The python is the most comment programming language used for data analytics. \"\n",
    "document2 = \"Most popular programming languages that use in data science are python and R programming.\"\n",
    "document3 = \"a lot of users used R programming \"\n",
    "document4 =\"Best data analytics languages is Python \"\n",
    "document5 = \"Python is an extremely popular general purpose.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms cannot work with raw text directly. Rather, the text must be converted into vectors of numbers. In natural language processing, a common technique for extracting features from text is to place all of the words that occur in the text in a bucket. This aproach is called a bag of words model or BoW for short. It’s referred to as a “bag” of words because any information about the structure of the sentence is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstDoc = document1.split(' ')\n",
    "SecondDoc = document2.split(' ')\n",
    "ThirdDoc = document3.split(' ')\n",
    "FourthDoc = document4.split(' ')\n",
    "FifthDoc = document5.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'analytics.', '', 'most', 'used', 'for', 'Most', 'and', 'the', 'use', 'python', 'languages', 'that', 'comment', 'The', 'in', 'science', 'programming.', 'popular', 'R', 'are', 'data', 'language', 'programming'}\n",
      "{'is', 'analytics.', '', 'most', 'used', 'for', 'Most', 'and', 'the', 'use', 'python', 'languages', 'that', 'comment', 'The', 'in', 'science', 'programming.', 'popular', 'R', 'are', 'data', 'language', 'programming'}\n",
      "{'is', 'analytics.', '', 'most', 'used', 'purpose.', 'for', 'an', 'the', 'python', 'extremely', 'general', 'comment', 'The', 'popular', 'data', 'language', 'programming', 'Python'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "uniqueWords1 = set(FirstDoc).union(set(SecondDoc))\n",
    "print(uniqueWords)\n",
    "\n",
    "uniqueWords2 = set(ThirdDoc).union(set(FourthDoc)) \n",
    "print(uniqueWords)\n",
    "\n",
    "\n",
    "uniqueWords3 = set(FirstDoc).union(set(FifthDoc))\n",
    "print(uniqueWords3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'python', 'is', 'the', 'most', 'comment', 'programming', 'language', 'used', 'for', 'data', 'analytics.', '', 'Most', 'popular', 'programming', 'languages', 'that', 'use', 'in', 'data', 'science', 'are', 'python', 'and', 'R', 'programming.', 'a', 'lot', 'of', 'users', 'used', 'R', 'programming', '', 'Best', 'data', 'analytics', 'languages', 'is', 'Python', '', 'Python', 'is', 'an', 'extremely', 'popular', 'general', 'purpose.']\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = FirstDoc+ SecondDoc + ThirdDoc +FourthDoc +FifthDoc \n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nnumOfWords = dict.fromkeys(uniqueWords, 0)\\nfor i in uniqueWords:\\n    numOfWords[i] += 1\\n\\nnumOfWords2 = dict.fromkeys(uniqueWords2, 0)\\nfor j in SecondDoc:\\n    numOfWords2[j] += 1\\n    \\nnumOfWords4 = dict.fromkeys(uniqueWords4, 0)\\nfor k in FourthWords:\\n    numOfWords4[k] += 1  \\n    \\n'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "numOfWords = dict.fromkeys(uniqueWords, 0)\n",
    "for i in uniqueWords:\n",
    "    numOfWords[i] += 1\n",
    "\n",
    "numOfWords2 = dict.fromkeys(uniqueWords2, 0)\n",
    "for j in SecondDoc:\n",
    "    numOfWords2[j] += 1\n",
    "    \n",
    "numOfWords4 = dict.fromkeys(uniqueWords4, 0)\n",
    "for k in FourthWords:\n",
    "    numOfWords4[k] += 1  \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWords = dict.fromkeys(uniqueWords, 0)\n",
    "for i in uniqueWords:\n",
    "    numOfWords[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'python': 2, 'is': 3, 'the': 1, 'most': 1, 'comment': 1, 'programming': 3, 'language': 1, 'used': 2, 'for': 1, 'data': 3, 'analytics.': 1, '': 3, 'Most': 1, 'popular': 2, 'languages': 2, 'that': 1, 'use': 1, 'in': 1, 'science': 1, 'are': 1, 'and': 1, 'R': 2, 'programming.': 1, 'a': 1, 'lot': 1, 'of': 1, 'users': 1, 'Best': 1, 'analytics': 1, 'Python': 2, 'an': 1, 'extremely': 1, 'general': 1, 'purpose.': 1}\n"
     ]
    }
   ],
   "source": [
    "print(numOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"      \\nfor word in FirstDoc:\\n\\n    numOfWords1.setdefault(word, 0)\\n\\n    numOfWords1[word] += 1  \\n    \\n    \\nfor word in SecondDoc:\\n\\n    numOfWords2.setdefault(word, 0)\\n\\n    numOfWords1[word] += 1  \\n\\nfor word in ThirdDoc:\\n\\n    numOfWords3.setdefault(word, 0)\\n\\n    numOfWords3[word] += 1 \\n    \\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "\"\"\"\"      \n",
    "for word in FirstDoc:\n",
    "\n",
    "    numOfWords1.setdefault(word, 0)\n",
    "\n",
    "    numOfWords1[word] += 1  \n",
    "    \n",
    "    \n",
    "for word in SecondDoc:\n",
    "\n",
    "    numOfWords2.setdefault(word, 0)\n",
    "\n",
    "    numOfWords1[word] += 1  \n",
    "\n",
    "for word in ThirdDoc:\n",
    "\n",
    "    numOfWords3.setdefault(word, 0)\n",
    "\n",
    "    numOfWords3[word] += 1 \n",
    "    \n",
    "\"\"\"\"\"     \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'setdefault'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11180/1454486300.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muniqueWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnumOfWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnumOfWords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'setdefault'"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "for word in uniqueWords:\n",
    "    numOfWords.setdefault(word, 0)\n",
    "    numOfWords[word] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 11, 'analytics.': 11, '': 11, 'most': 11, 'used': 11, 'for': 11, 'Most': 101, 'and': 3, 'the': 11, 'use': 3, 'python': 14, 'languages': 3, 'that': 3, 'comment': 11, 'The': 11, 'in': 3, 'science': 3, 'programming.': 3, 'popular': 3, 'R': 3, 'are': 3, 'data': 14, 'language': 11, 'programming': 14}\n",
      "{'': 0, 'is': 0, 'analytics': 0, 'a': 0, 'lot': 0, 'used': 0, 'Best': 0, 'users': 0, 'of': 0, 'R': 0, 'languages': 0, 'data': 0, 'programming': 0, 'Python': 0, 'Most': 0, 'popular': 0, 'that': 0, 'use': 0, 'in': 0, 'science': 0, 'are': 0, 'python': 0, 'and': 0, 'programming.': 0}\n"
     ]
    }
   ],
   "source": [
    "print(numOfWords1)\n",
    "print(numOfWords2)\n",
    "#print(numOfWords3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem with the bag of words approach is that it doesn’t account for noise. In other words, certain words are used to formulate sentences but do not add any semantic meaning to the text. For example, the most commonly used word in the english language is the which represents 7% of all words written or spoken. You couldn’t make deduce anything about a text given the fact that it contains the word the. On the other hand, words like good and awesome could be used to determine whether a rating was positive or not.\n",
    "\n",
    "In natural language processing, useless words are referred to as stop words. The python natural language toolkit library provides a list of english stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency (TF)\n",
    "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency.\n",
    "<img src=\"tf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0.07692307692307693, 'analytics.': 0.07692307692307693, '': 0.07692307692307693, 'most': 0.07692307692307693, 'used': 0.07692307692307693, 'for': 0.07692307692307693, 'Most': 0.0, 'and': 0.0, 'the': 0.07692307692307693, 'use': 0.0, 'python': 0.07692307692307693, 'languages': 0.0, 'that': 0.0, 'comment': 0.07692307692307693, 'The': 0.07692307692307693, 'in': 0.0, 'science': 0.0, 'programming.': 0.0, 'popular': 0.0, 'R': 0.0, 'are': 0.0, 'data': 0.07692307692307693, 'language': 0.07692307692307693, 'programming': 0.07692307692307693}\n",
      "{'is': 0.07692307692307693, 'analytics.': 0.07692307692307693, '': 0.07692307692307693, 'most': 0.07692307692307693, 'used': 0.07692307692307693, 'for': 0.07692307692307693, 'Most': 0.0, 'and': 0.0, 'the': 0.07692307692307693, 'use': 0.0, 'python': 0.07692307692307693, 'languages': 0.0, 'that': 0.0, 'comment': 0.07692307692307693, 'The': 0.07692307692307693, 'in': 0.0, 'science': 0.0, 'programming.': 0.0, 'popular': 0.0, 'R': 0.0, 'are': 0.0, 'data': 0.07692307692307693, 'language': 0.07692307692307693, 'programming': 0.07692307692307693}\n"
     ]
    }
   ],
   "source": [
    "print(tfA)\n",
    "print(tfA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Data Frequency (IDF)\n",
    "The log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus.\n",
    "<img src=\"idf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA, numOfWordsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0.6931471805599453, 'analytics.': 0.6931471805599453, '': 0.6931471805599453, 'most': 0.6931471805599453, 'used': 0.6931471805599453, 'for': 0.6931471805599453, 'Most': 0.6931471805599453, 'and': 0.6931471805599453, 'the': 0.6931471805599453, 'use': 0.6931471805599453, 'python': 0.0, 'languages': 0.6931471805599453, 'that': 0.6931471805599453, 'comment': 0.6931471805599453, 'The': 0.6931471805599453, 'in': 0.6931471805599453, 'science': 0.6931471805599453, 'programming.': 0.6931471805599453, 'popular': 0.6931471805599453, 'R': 0.6931471805599453, 'are': 0.6931471805599453, 'data': 0.0, 'language': 0.6931471805599453, 'programming': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the TF-IDF is simply the TF multiplied by IDF.\n",
    "<img src=\"tfidf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    \n",
    "    \n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "        \n",
    "        \n",
    "        \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0.053319013889226566, 'analytics.': 0.053319013889226566, '': 0.053319013889226566, 'most': 0.053319013889226566, 'used': 0.053319013889226566, 'for': 0.053319013889226566, 'Most': 0.0, 'and': 0.0, 'the': 0.053319013889226566, 'use': 0.0, 'python': 0.0, 'languages': 0.0, 'that': 0.0, 'comment': 0.053319013889226566, 'The': 0.053319013889226566, 'in': 0.0, 'science': 0.0, 'programming.': 0.0, 'popular': 0.0, 'R': 0.0, 'are': 0.0, 'data': 0.0, 'language': 0.053319013889226566, 'programming': 0.0}\n",
      "{'is': 0.0, 'analytics.': 0.0, '': 0.0, 'most': 0.0, 'used': 0.0, 'for': 0.0, 'Most': 0.049510512897138946, 'and': 0.049510512897138946, 'the': 0.0, 'use': 0.049510512897138946, 'python': 0.0, 'languages': 0.049510512897138946, 'that': 0.049510512897138946, 'comment': 0.0, 'The': 0.0, 'in': 0.049510512897138946, 'science': 0.049510512897138946, 'programming.': 0.049510512897138946, 'popular': 0.049510512897138946, 'R': 0.049510512897138946, 'are': 0.049510512897138946, 'data': 0.0, 'language': 0.0, 'programming': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(tfidfA)\n",
    "print(tfidfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>analytics.</th>\n",
       "      <th></th>\n",
       "      <th>most</th>\n",
       "      <th>used</th>\n",
       "      <th>for</th>\n",
       "      <th>Most</th>\n",
       "      <th>and</th>\n",
       "      <th>the</th>\n",
       "      <th>use</th>\n",
       "      <th>...</th>\n",
       "      <th>The</th>\n",
       "      <th>in</th>\n",
       "      <th>science</th>\n",
       "      <th>programming.</th>\n",
       "      <th>popular</th>\n",
       "      <th>R</th>\n",
       "      <th>are</th>\n",
       "      <th>data</th>\n",
       "      <th>language</th>\n",
       "      <th>programming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         is  analytics.                most      used       for      Most  \\\n",
       "0  0.053319    0.053319  0.053319  0.053319  0.053319  0.053319  0.000000   \n",
       "1  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.049511   \n",
       "\n",
       "        and       the       use  ...       The        in   science  \\\n",
       "0  0.000000  0.053319  0.000000  ...  0.053319  0.000000  0.000000   \n",
       "1  0.049511  0.000000  0.049511  ...  0.000000  0.049511  0.049511   \n",
       "\n",
       "   programming.   popular         R       are  data  language  programming  \n",
       "0      0.000000  0.000000  0.000000  0.000000   0.0  0.053319          0.0  \n",
       "1      0.049511  0.049511  0.049511  0.049511   0.0  0.000000          0.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
